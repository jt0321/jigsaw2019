{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(666)\n",
    "np.random.seed(666)\n",
    "mx.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolingLayer(gluon.HybridBlock):\n",
    "    \"\"\"A block for mean pooling of encoder features\"\"\"\n",
    "    def __init__(self, prefix=None, params=None):\n",
    "        super(MeanPoolingLayer, self).__init__(prefix=prefix, params=params)\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length): # pylint: disable=arguments-differ\n",
    "        \"\"\"Forward logic\"\"\"\n",
    "        # Data will have shape (T, N, C)\n",
    "        masked_encoded = F.SequenceMask(data,\n",
    "                                        sequence_length=valid_length,\n",
    "                                        use_sequence_length=True)\n",
    "        agg_state = F.broadcast_div(F.sum(masked_encoded, axis=0),\n",
    "                                    F.expand_dims(valid_length, axis=1))\n",
    "        return agg_state\n",
    "\n",
    "\n",
    "class SentimentNet(gluon.HybridBlock):\n",
    "    \"\"\"Network for sentiment analysis.\"\"\"\n",
    "    def __init__(self, dropout, prefix=None, params=None):\n",
    "        super(SentimentNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            self.embedding = None # will set with lm embedding later\n",
    "            self.encoder = None # will set with lm encoder later\n",
    "            self.agg_layer = MeanPoolingLayer()\n",
    "            self.output = gluon.nn.HybridSequential()\n",
    "            with self.output.name_scope():\n",
    "                self.output.add(gluon.nn.Dropout(dropout))\n",
    "                self.output.add(gluon.nn.Dense(1, flatten=False))\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length): # pylint: disable=arguments-differ\n",
    "        encoded = self.encoder(self.embedding(data))  # Shape(T, N, C)\n",
    "        agg_state = self.agg_layer(encoded, valid_length)\n",
    "        out = self.output(agg_state)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0\n",
    "language_model_name = 'standard_lstm_lm_200'\n",
    "pretrained = True\n",
    "learning_rate, batch_size = 0.00025, 128\n",
    "bucket_num, bucket_ratio = 10, 0.2\n",
    "epochs = 5\n",
    "grad_clip = None\n",
    "log_interval = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model, vocab = nlp.model.get_model(name=language_model_name,\n",
    "                                      dataset_name='wikitext-2',\n",
    "                                      pretrained=pretrained,\n",
    "                                      ctx=context,\n",
    "                                      dropout=dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SA model from pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentNet(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2)\n",
      "  (agg_layer): MeanPoolingLayer(\n",
      "  \n",
      "  )\n",
      "  (output): HybridSequential(\n",
      "    (0): Dropout(p = 0, axes=())\n",
      "    (1): Dense(None -> 1, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = SentimentNet(dropout=dropout)\n",
    "net.embedding = lm_model.embedding\n",
    "net.encoder = lm_model.encoder\n",
    "net.hybridize()\n",
    "net.output.initialize(mx.init.Xavier(), ctx=context)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test.csv', 'tmp', 'train.csv', 'sample_submission.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('data/jigsaw')\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = path/'train.csv'\n",
    "test_csv = path/'test.csv'\n",
    "sample_csv = path/'sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['comment_text'] = train_df['comment_text'].astype(str)\n",
    "train_df['target']=(train_df['target']>=0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw = train_df[['comment_text', 'target']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['comment_text'] = test_df['comment_text'].astype(str)\n",
    "test_data_raw = test_df[['comment_text']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Jeff Sessions is another one of Trump's Orwellian choices. He believes and has believed his entire career the exact opposite of what the position requires.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_raw[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer takes as input a string and outputs a list of tokens.\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "\n",
    "# length_clip takes as input a list and outputs a list with maximum length 500.\n",
    "length_clip = nlp.data.ClipSequence(500)\n",
    "\n",
    "def preprocess(x):\n",
    "    data, label = x\n",
    "    # A token index or a list of token indices is\n",
    "    # returned according to the vocabulary.\n",
    "    data = vocab[length_clip(tokenizer(data))]\n",
    "    return data, label\n",
    "\n",
    "def test_preprocess(x):\n",
    "    data = x[0]\n",
    "    # A token index or a list of token indices is\n",
    "    # returned according to the vocabulary.\n",
    "    data = vocab[length_clip(tokenizer(data))]\n",
    "    return data\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "pool = mp.Pool()\n",
    "# Each sample is processed in an asynchronous manner.\n",
    "train_dataset = gluon.data.SimpleDataset(pool.map(preprocess, train_data_raw))\n",
    "train_data_lengths = gluon.data.SimpleDataset(pool.map(get_length, train_dataset))\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=129.93s, #Sentences=1804874\n"
     ]
    }
   ],
   "source": [
    "print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = gluon.data.SimpleDataset(pool.map(test_preprocess, test_data_raw))\n",
    "#test_dataset = gluon.data.SimpleDataset(test_data_raw)\n",
    "#test_dataset = test_dataset.transform(test_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=1804874, batch_num=10121\n",
      "  key=[50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
      "  cnt=[1020105, 421486, 178152, 131888, 52962, 253, 16, 8, 2, 2]\n",
      "  batch_size=[256, 128, 128, 128, 128, 128, 128, 128, 128, 128]\n"
     ]
    }
   ],
   "source": [
    "batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(axis=0, ret_length=True),\n",
    "    nlp.data.batchify.Stack(dtype='float32'))\n",
    "batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "    train_data_lengths,\n",
    "    batch_size=batch_size,\n",
    "    num_buckets=bucket_num,\n",
    "    ratio=bucket_ratio,\n",
    "    shuffle=True)\n",
    "print(batch_sampler.stats())\n",
    "train_dataloader = gluon.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_sampler=batch_sampler,\n",
    "    batchify_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batchify_fn = nlp.data.batchify.Pad(axis=0, ret_length=True)\n",
    "test_dataloader = gluon.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    batchify_fn=test_batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Evaluation using loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'ftml',\n",
    "                        {'learning_rate': learning_rate})\n",
    "loss = gluon.loss.SigmoidBCELoss()\n",
    "\n",
    "parameters = net.collect_params().values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='avg loss: 0.000000', max=13163, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 10000/13163] elapsed 220.42 s, avg loss 0.002249, throughput 336.11K wps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='avg loss: 0.000000', max=13163, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 10000/13163] elapsed 221.57 s, avg loss 0.002069, throughput 333.60K wps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='avg loss: 0.000000', max=13163, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 Batch 10000/13163] elapsed 222.49 s, avg loss 0.001924, throughput 333.79K wps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='avg loss: 0.000000', max=13163, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 Batch 10000/13163] elapsed 220.90 s, avg loss 0.001815, throughput 332.99K wps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='avg loss: 0.000000', max=13163, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 10000/13163] elapsed 222.20 s, avg loss 0.001689, throughput 333.52K wps\n"
     ]
    }
   ],
   "source": [
    "# Training/Testing\n",
    "for epoch in range(epochs):\n",
    "    # Epoch training stats\n",
    "    start_epoch_time = time.time()\n",
    "    epoch_L = 0.0\n",
    "    epoch_sent_num = 0\n",
    "    epoch_wc = 0\n",
    "    # Log interval training stats\n",
    "    start_log_interval_time = time.time()\n",
    "    log_interval_wc = 0\n",
    "    log_interval_sent_num = 0\n",
    "    log_interval_L = 0.0\n",
    "\n",
    "    for i, ((data, length), label) in tqdm_notebook(enumerate(train_dataloader), total=len(train_dataloader), leave=False):\n",
    "        L = 0\n",
    "        wc = length.sum().asscalar()\n",
    "        log_interval_wc += wc\n",
    "        epoch_wc += wc\n",
    "        log_interval_sent_num += data.shape[1]\n",
    "        epoch_sent_num += data.shape[1]\n",
    "        with autograd.record():\n",
    "            output = net(data.as_in_context(context).T,\n",
    "                         length.as_in_context(context)\n",
    "                               .astype(np.float32))\n",
    "            L = L + loss(output, label.as_in_context(context)).mean()\n",
    "        L.backward()\n",
    "        # Clip gradient\n",
    "        if grad_clip:\n",
    "            gluon.utils.clip_global_norm(\n",
    "                [p.grad(context) for p in parameters],\n",
    "                grad_clip)\n",
    "        # Update parameter\n",
    "        trainer.step(1)\n",
    "        log_interval_L += L.asscalar()\n",
    "        epoch_L += L.asscalar()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print(\n",
    "                '[Epoch {} Batch {}/{}] elapsed {:.2f} s, '\n",
    "                'avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                    epoch, i + 1, len(train_dataloader),\n",
    "                    time.time() - start_log_interval_time,\n",
    "                    log_interval_L / log_interval_sent_num, log_interval_wc\n",
    "                    / 1000 / (time.time() - start_log_interval_time)))\n",
    "            # Clear log interval training stats\n",
    "            start_log_interval_time = time.time()\n",
    "            log_interval_wc = 0\n",
    "            log_interval_sent_num = 0\n",
    "            log_interval_L = 0\n",
    "    end_epoch_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_parameters('lm-20190624-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_parameters('lm-20190624-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.00679768]]\n",
       "<NDArray 1x1 @gpu(0)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(\n",
    "    mx.nd.reshape(\n",
    "        mx.nd.array(vocab[['This', 'movie', 'is', 'amazing']], ctx=context),\n",
    "        shape=(-1, 1)), mx.nd.array([4], ctx=context)).sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=761), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "results = []\n",
    "for _, (data, length) in tqdm_notebook(enumerate(test_dataloader), total=len(test_dataloader),leave=False):\n",
    "    output = net(data.as_in_context(context).T,\n",
    "                 length.as_in_context(context)\n",
    "                       .astype(np.float32))\n",
    "    results.extend(out for out in output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975501"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[4].sigmoid().asnumpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [result.sigmoid().asnumpy()[0] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_df['id'],\n",
    "    'prediction': predictions\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
